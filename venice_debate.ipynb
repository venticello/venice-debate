{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/samunderSingh12/debate_baby/blob/main/debate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","source":["# @title 1. Install Libraries\n","!pip install -q gradio openai\n"],"metadata":{"id":"Z_vv2AzCIC_j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748586463775,"user_tz":-180,"elapsed":31032,"user":{"displayName":"Anton","userId":"14822071739583504913"}},"outputId":"ae2c0b56-2d4d-47a7-85e0-7155a999d9d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# @title 2. Import Libraries and Define Functions\n","import re\n","import gradio as gr\n","from gradio import ChatMessage\n","import openai\n","import time\n","import textwrap\n","from dataclasses import dataclass\n","from typing import Any, Generator\n","\n","\n","DEFAULT_PROTAGONIST_MODEL = \"venice-uncensored\"\n","DEFAULT_ANTAGONIST_MODEL = \"llama-3.1-405b\"\n","\n","DEFAULT_MAX_TOKENS = 700\n","DEFAULT_DEBATE_TURNS = 3\n","TEMPERATURE = 0.6\n","\n","THINKING_ANIMATION = [\"Thinking.\", \"Thinking..\", \"Thinking...\"]\n","MODELS = [\n","    \"venice-uncensored\",\n","    \"qwen3-235b\",\n","    \"deepseek-r1-671b\",\n","    \"llama-3.1-405b\",\n","    \"llama-3.3-70b\",\n","    \"dolphin-2.9.2-qwen2-72b\",\n","    \"qwen-2.5-qwq-32b\",\n","    \"mistral-31-24b\",\n","]\n","\n","DEFAULT_PROTAGONIST_SYS_PROMPT = (\n","    \"You are a precise and analytical AI debater representing the Protagonist perspective. \"\n","    \"Engage directly with your opponent's arguments, referencing specific points they've made throughout the debate where relevant. \"\n","    \"Maintain a logical flow and build upon your previous arguments. Your goal is a constructive exchange of ideas.\"\n",")\n","DEFAULT_ANTAGONIST_SYS_PROMPT = (\n","    \"You are a creative and insightful AI debater representing the Antagonist perspective. \"\n","    \"Challenge your opponent's points thoughtfully and connect your arguments back to the core topic. \"\n","    \"Feel free to refer to earlier statements in the debate to highlight consistencies or contradictions. Aim for a compelling and engaging discussion.\"\n",")\n","\n","\n","@dataclass\n","class DebateParticipant:\n","    model_name: str\n","    system_prompt: str\n","    is_protagonist: bool\n","\n","    @property\n","    def display_name(self) -> str:\n","        role = \"Protagonist\" if self.is_protagonist else \"Antagonist\"\n","        return f\"{role} ({self.model_name})\"\n","\n","\n","\n","def get_venice_client(api_key: str) -> openai.OpenAI:\n","    \"\"\"Initialize and validate Venice API client.\"\"\"\n","    if not api_key:\n","        raise gr.Error(\"Venice API Key is missing!\")\n","    try:\n","        client = openai.OpenAI(api_key=api_key, base_url=\"https://api.venice.ai/api/v1\")\n","        client.models.list()\n","        return client\n","    except openai.AuthenticationError:\n","        raise gr.Error(\"Invalid Venice API Key.\")\n","    except Exception as e:\n","        raise gr.Error(f\"Venice Client Error: {e}\")\n","\n","\n","def validate_inputs(topic: str, venice_key: str, max_tokens: int) -> None:\n","    \"\"\"Validate all input parameters.\"\"\"\n","    if not topic:\n","        raise gr.Error(\"Please provide a debate topic!\")\n","    if not venice_key:\n","        raise gr.Error(\"Please provide your Venice API Key!\")\n","    try:\n","        if max_tokens <= 0:\n","            raise ValueError(\"Max tokens must be positive.\")\n","    except (ValueError, TypeError):\n","        raise gr.Error(\"Invalid Max Tokens value. Please use the slider or enter a positive number.\")\n","\n","\n","def create_debate_header(topic: str, protagonist: DebateParticipant, antagonist: DebateParticipant,\n","                         max_tokens: int) -> str:\n","    \"\"\"Create the initial debate transcript header.\"\"\"\n","    return (\n","        f\"## Debate Topic: {topic}\\n\\n\"\n","        f\"**Settings:**\\n\"\n","        f\"- Protagonist Model: `{protagonist.model_name}`\\n\"\n","        f\"- Antagonist Model: `{antagonist.model_name}`\\n\"\n","        f\"- Max Tokens: {max_tokens}\\n\"\n","        f\"- Protagonist Persona: *{textwrap.shorten(protagonist.system_prompt, 128)}*\\n\"\n","        f\"- Antagonist Persona: *{textwrap.shorten(antagonist.system_prompt, 128)}*\\n\\n\"\n","        f\"---\\n\\n\"\n","    )\n","\n","\n","def process_streaming_response(\n","        stream: Any,\n","        debate_transcript: str,\n","        status_message: str\n",") -> Generator[tuple[str, str], None, tuple[str, str]]:\n","    \"\"\"Process streaming response and update transcript.\"\"\"\n","    full_response = \"\"\n","    in_think_block = False\n","\n","    for chunk in stream:\n","        if not hasattr(chunk, 'choices') or not chunk.choices:\n","            continue\n","        if not hasattr(chunk.choices[0], 'delta') or not hasattr(chunk.choices[0].delta, 'content'):\n","            continue\n","\n","        content = chunk.choices[0].delta.content\n","        if content is None:\n","            continue\n","\n","        full_response += content\n","\n","        # Handle think blocks\n","        if \"<think>\" in content:\n","            in_think_block = True\n","            debate_transcript += \"<blockquote>\"\n","            content = content.replace(\"<think>\", \"\")\n","        elif \"</think>\" in content:\n","            in_think_block = False\n","            content = content.replace(\"</think>\", \"\")\n","            debate_transcript += content + \"</blockquote>\"\n","            continue\n","\n","        debate_transcript += content\n","        yield debate_transcript, status_message\n","\n","    return full_response, debate_transcript\n","\n","\n","def run_debate(\n","        topic: str,\n","        venice_key: str,\n","        protagonist_model_name: str,\n","        antagonist_model_name: str,\n","        protagonist_system_prompt: str,\n","        antagonist_system_prompt: str,\n","        max_tokens_input: int\n",") -> Generator[tuple[str, str], None, None]:\n","    \"\"\"Main debate function that orchestrates the conversation.\"\"\"\n","    validate_inputs(topic, venice_key, max_tokens_input)\n","\n","    try:\n","        yield \"Initializing Client...\", \"\"\n","        venice_client = get_venice_client(venice_key)\n","    except gr.Error as e:\n","        yield f\"Initialization Error: {e}\", \"Error\"\n","        return\n","\n","    protagonist = DebateParticipant(\n","        model_name=protagonist_model_name,\n","        system_prompt=protagonist_system_prompt,\n","        is_protagonist=True\n","    )\n","    antagonist = DebateParticipant(\n","        model_name=antagonist_model_name,\n","        system_prompt=antagonist_system_prompt,\n","        is_protagonist=False\n","    )\n","\n","    conversation_history: List[Dict[str, str]] = []\n","    debate_transcript = create_debate_header(topic, protagonist, antagonist, max_tokens_input)\n","\n","    try:\n","        for turn in range(DEFAULT_DEBATE_TURNS * 2):\n","            current_participant = protagonist if turn % 2 == 0 else antagonist\n","            status_message = f\"Turn {turn // 2 + 1} / {DEFAULT_DEBATE_TURNS} - {current_participant.display_name} {THINKING_ANIMATION[turn % 3]}\"\n","            print(f\"--- {status_message} ---\")\n","            yield debate_transcript, status_message\n","\n","            # Prepare message\n","            if turn == 0:\n","                current_user_instruction_text = f\"Begin the debate by presenting your opening statement on the topic: '{topic}'.\"\n","            else:\n","                if not conversation_history:\n","                    raise gr.Error(\"Unexpected error: Conversation history is empty\")\n","                last_message_content = conversation_history[-1]['content']\n","                current_user_instruction_text = (\n","                    f\"Considering the debate history so far, present your response to the opponent's previous statement. \"\n","                    f\"Opponent's statement: '{textwrap.shorten(last_message_content, width=150, placeholder='...')}'\"\n","                )\n","\n","            messages = [\n","                {\"role\": \"system\", \"content\": current_participant.system_prompt},\n","                *conversation_history,\n","                {\"role\": \"user\", \"content\": current_user_instruction_text}\n","            ]\n","\n","            try:\n","                print(f\"Sending {len(messages)} messages to {current_participant.display_name}\")\n","\n","                stream = venice_client.chat.completions.create(\n","                    model=current_participant.model_name,\n","                    messages=messages,\n","                    max_tokens=max_tokens_input,\n","                    temperature=TEMPERATURE,\n","                    stream=True\n","                )\n","\n","                debate_transcript += f\"**{current_participant.display_name}:**\\n\"\n","\n","                full_response, debate_transcript = yield from process_streaming_response(\n","                    stream, debate_transcript, status_message\n","                )\n","\n","                # Update conversation history\n","                conversation_history.append({\"role\": \"user\", \"content\": current_user_instruction_text})\n","                ai_response_cleaned_text = re.sub(r'<think>.*?</think>', '', full_response, flags=re.DOTALL)\n","                conversation_history.append({\"role\": \"assistant\", \"content\": ai_response_cleaned_text})\n","\n","                debate_transcript += \"\\n\\n---\\n\\n\"\n","                time.sleep(1.4)\n","\n","            except Exception as e:\n","                error_detail = str(e)\n","                if \"AuthenticationError\" in error_detail:\n","                    error_message = f\"API Auth Error ({current_participant.display_name}). Check Key.\"\n","                elif \"RateLimitError\" in error_detail:\n","                    error_message = f\"Rate Limit Error ({current_participant.display_name}). Wait & retry.\"\n","                elif \"NotFoundError\" in error_detail and \"model\" in error_detail:\n","                    error_message = f\"Model Not Found ({current_participant.model_name}). Check Name/Access.\"\n","                elif (\"BadRequestError\" in error_detail and \"context_length\" in error_detail) or \\\n","                        (\"invalid_request_error\" in error_detail and \"maximum context length\" in error_detail.lower()):\n","                    error_message = f\"Context Length Exceeded ({current_participant.display_name}). Reduce turns/max_tokens or use a model with larger context.\"\n","                else:\n","                    error_message = f\"API Error ({current_participant.display_name}): Check console.\"\n","\n","                error_log_message = f\"\\n\\n**API Error during {current_participant.display_name}'s turn:** {e}\\nDebate halted.\"\n","                print(error_log_message)\n","                debate_transcript += f\"**SYSTEM:**\\n*{error_message}*\\n\\n---\\n\\n\"\n","                yield debate_transcript, f\"Error: {current_participant.display_name}\"\n","                return\n","\n","        status = \"Debate Complete!\"\n","        debate_transcript += f\"**{status}**\"\n","        print(status)\n","        yield debate_transcript, status\n","\n","    except Exception as e:\n","        error_message = f\"\\n\\n**An unexpected error occurred:** {e}\\nDebate halted.\"\n","        print(error_message)\n","        debate_transcript += f\"**SYSTEM:**\\n*An unexpected error occurred: {e}*\\n\\n---\\n\\n\"\n","        yield debate_transcript, \"An unexpected error occurred.\"\n"],"metadata":{"id":"qMG1J4m2PLJY","executionInfo":{"status":"ok","timestamp":1748986822659,"user_tz":-180,"elapsed":14,"user":{"displayName":"Anton","userId":"14822071739583504913"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# @title 3. Create and Launch Gradio Interface\n","from google.colab import userdata\n","\n","\n","# Clear any previous Gradio launches\n","gr.close_all()\n","\n","# Define the Gradio Interface\n","with gr.Blocks(title=\"Debate - Venice AI\",\n","               theme=gr.themes.Base(primary_hue=\"orange\",\n","                                    font=[gr.themes.GoogleFont(\"Aeonik Fono\"), \"Arial\", \"sans-serif\"])\n","                       .set(block_label_text_color=\"black\",\n","                            block_label_text_color_dark=\"black\",\n","                            ),\n","               css=\"\"\"\n","    .settings-container {\n","        background: linear-gradient(135deg, #BEA989 0%, #EEEDE4 100%);\n","        border-radius: 15px;\n","        padding: 20px;\n","        color: white;\n","    }\n","    .gallery-item {\n","        background-color: var(--button-primary-background-fill);\n","    }\n","    .gallery.selected {\n","        background-color: var(--button-primary-background-fill-hover);\n","    }\n","    .chat-container {\n","        padding-top: 10px;\n","    }\n","    #example-debate-topics.label {\n","        color: black!important;\n","    }\n","    \"\"\") as demo:\n","    gr.Markdown(\n","        \"\"\"\n","        # [<img src=\"https://venice.ai/images/icon-192.png\" width=\"64\"/>](https://venice.ai/images/icon-192.png) Venice Debate\n","\n","        Set debate topic, API keys, models, max tokens, and custom system prompts (personas) for each AI.\n","        The prompts now encourage deeper engagement with the debate history for a more immersive experience.\n","        **Note:** Context limits can still be reached in long debates. Venice usage incurs costs.\n","        \"\"\"\n","    )\n","\n","    with gr.Row():\n","        # Left column - Settings\n","        with gr.Column(scale=1, elem_classes=\"settings-container\"):\n","            topic_input = gr.Textbox(\n","                label=\"Debate Topic\", placeholder=\"e.g., Should AI be uncensorable?\", lines=2\n","            )\n","\n","            # Examples\n","            gr.Examples(\n","                examples=[\n","                    [\"Is anonymity on the internet a right or a privilege?\"],\n","                    [\n","                        \"Will the evolution of AI lead to more decentralized, privacy-focused models, or will centralized control become inevitable?\"],\n","                    [\n","                        \"Should AI models be allowed to use personal data without explicit consent to drive technological advancement?\"]\n","                ],\n","                inputs=[topic_input], label=\"Example Debate Topics\",\n","                elem_id='example-debate-topics'\n","            )\n","\n","\n","            gr.Markdown(\"### Credentials\", visible=not bool(userdata.get('VENICE_KEY')))\n","\n","            venice_key_input = gr.Textbox(\n","                label=\"Venice API Key\",\n","                type=\"password\",\n","                placeholder=\"venice_api_key...\",\n","                value=userdata.get('VENICE_KEY'),\n","                visible=not bool(userdata.get('VENICE_KEY'))\n","            )\n","\n","            gr.Markdown(\"### Model Selection\")\n","            protagonist_model_input = gr.Dropdown(\n","                MODELS,\n","                value=DEFAULT_PROTAGONIST_MODEL,\n","                label=\"Protagonist Model Name\"\n","            )\n","            antagonist_model_input = gr.Dropdown(\n","                MODELS,\n","                value=DEFAULT_ANTAGONIST_MODEL,\n","                label=\"Antagonist Model Name\"\n","            )\n","\n","            gr.Markdown(\"### Persona / System Prompts\")\n","            protagonist_system_prompt_input = gr.Textbox(\n","                label=\"Protagonist System Prompt\",\n","                placeholder=\"Define persona/role\",\n","                value=DEFAULT_PROTAGONIST_SYS_PROMPT,\n","                lines=4\n","            )\n","            antagonist_system_prompt_input = gr.Textbox(\n","                label=\"Antagonist System Prompt\",\n","                placeholder=\"Define persona/role\",\n","                value=DEFAULT_ANTAGONIST_SYS_PROMPT,\n","                lines=4\n","            )\n","\n","            gr.Markdown(\"### Debate Settings\")\n","            max_tokens_slider = gr.Slider(\n","                label=\"Max Tokens per Turn\",\n","                minimum=50,\n","                maximum=1700,\n","                step=10,\n","                value=DEFAULT_MAX_TOKENS\n","            )\n","\n","            status_output = gr.Textbox(\n","                label=\"Status\",\n","                placeholder=\"Waiting to start...\",\n","                interactive=False\n","            )\n","            start_button = gr.Button(\"🚀 Start Debate\", variant=\"primary\")\n","\n","        # Right column - Chat\n","        with gr.Column(scale=2, elem_classes=\"chat-container\"):\n","            debate_output = gr.Markdown(\n","                label=\"Debate Transcript\",\n","                value=\"*Debate transcript will appear here...*\"\n","            )\n","\n","    # --- Button Click Actions ---\n","    start_button.click(\n","        fn=run_debate,\n","        inputs=[\n","            topic_input,\n","            venice_key_input,\n","            protagonist_model_input,\n","            antagonist_model_input,\n","            protagonist_system_prompt_input,\n","            antagonist_system_prompt_input,\n","            max_tokens_slider\n","        ],\n","        outputs=[debate_output, status_output]\n","    )\n","\n","# --- Launch App ---\n","demo.launch(share=True, debug=False, show_error=True, pwa=True)\n","\n","print(\"\\n✅ Gradio app launched!\")\n","print(\"👉 Click the 'Running on public URL' link above.\")\n","print(\"\\n⚠️ Context window limits are still a possibility in long debates. Monitor token usage if needed.\")\n"],"metadata":{"id":"I2AhlMoEPTvR","colab":{"base_uri":"https://localhost:8080/","height":698},"executionInfo":{"status":"ok","timestamp":1748986828948,"user_tz":-180,"elapsed":2704,"user":{"displayName":"Anton","userId":"14822071739583504913"}},"outputId":"f1153120-7d20-4cb7-a858-05e5f61c311e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://4513ba62651413454d.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://4513ba62651413454d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","✅ Gradio app launched!\n","👉 Click the 'Running on public URL' link above.\n","\n","⚠️ Context window limits are still a possibility in long debates. Monitor token usage if needed.\n"]}]}]}